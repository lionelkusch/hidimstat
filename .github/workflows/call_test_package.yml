# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Tests

on:
  # Allows you to run this workflow from another workflow
  workflow_call:

# permission for reporting
permissions:
  contents: read
  actions: read
  checks: write
  pull-requests: write


jobs:
  build-test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: true  # if on job fail all the other jobs are cancelled
      matrix:
        python-version: ["3.13", "3.12"] #CHECK: not forget to add new version of python
        os: [macos-latest, ubuntu-latest, windows-latest]
    
    # name for files
    env:
      REPORT_NAME: ${{ matrix.python-version }}-os-${{ matrix.os }}

    steps:
    # Checkout to the branch for getting the right code
    - uses: actions/checkout@v4
    # Install python
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    # Install dependency for running the tests
    - name: Install the dependance
      run: |
        python -m pip install --upgrade pip
        python -m pip install .[test]
    # Run the tests with coverage and report add this end
    # option for the coverage: --cov=. --cov-report=xml:... 
    # option for mardown report: --md-report --md-report-output= ...
    # option for html report: --self-contained-html --html=report-...
    # option for duration report: --pytest-durations= number of second --pytest-resultlog=log-...
    - name: Generate coverage report
      run: |
        python -m pytest\
         --cov=. --cov-report=xml:cov/cov-${{env.REPORT_NAME}}.xml\
         --md-report --md-report-output=test-results-${{env.REPORT_NAME}}.md\
         --self-contained-html --html=report-${{env.REPORT_NAME}}.html\
         --pytest-durations=10 --pytest-resultlog=log-${{env.REPORT_NAME}}.txt
    ## uploading results
    - name: HTML Preview
      id: html_preview
      uses: pavi2410/html-preview-action@v4
      with:
        html_file: report-${{env.REPORT_NAME}}.html
        job_summary: false
    - name: Upload HTML test result
      if: ${{ always() && !cancelled()}}
      id: html_report
      uses: actions/upload-artifact@v4
      with:
        name: pytest-results-html-${{env.REPORT_NAME}}
        path: report-${{env.REPORT_NAME}}.html
        if-no-files-found: error
    # add the link for html report to the mardown file
    - name: Save link HTML in file
      if: ${{ always() && !cancelled()}}
      run: 
        echo -e "\n [HTML summary]("${{ steps.html_report.outputs.artifact-url }}") [2](html_preview.outputs.url)" >> test-results-${{env.REPORT_NAME}}.md
    - name: Upload md test result 
      if: ${{ always() && !cancelled()}}
      uses: actions/upload-artifact@v4
      with:
        name: pytest-results-md-${{env.REPORT_NAME}}
        path: test-results-${{env.REPORT_NAME}}.md 
        if-no-files-found: error
    - name: Upload duration test result
      if: ${{ always() && !cancelled()}}
      uses: actions/upload-artifact@v4
      with:
        name: pytest-results-log-${{env.REPORT_NAME}}
        path: log-${{env.REPORT_NAME}}.txt
        if-no-files-found: error
    - name: Upload coverage data
      if: ${{ always() && !cancelled()}}
      uses: actions/upload-artifact@v4
      with:
        name: cov-results-${{env.REPORT_NAME}}
        path: cov/cov-${{env.REPORT_NAME}}.xml
        if-no-files-found: error

  merge_md_test_data:
    # for more details
    # https://github.com/thombashi/pytest-md-report
    needs: build-test
    runs-on: ubuntu-latest
    if: ${{ failure() }}
    steps:
    # get report from previous job 
    - name: Get data
      uses: actions/download-artifact@v4
      with:
        pattern: pytest-results-md-*-os-*
    # create a comment in the PR with the failling tests
    # for more details: https://github.com/marocchino/sticky-pull-request-comment
    - name: Render the report to the PR when tests fail
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: test-report
        recreate: true
        path: ./pytest-results-md-3.13-os-macos-latest/test-results-3.13-os-macos-latest.md     #CHECK: need to be updated when new version are available
    # create a comment in the summary with the failling tests
    - name: Output reports to the job summary when tests fail
      shell: bash
      run: |
        for folder in *;
        do 
          echo "<details><summary>Failed Test Report: $folder</summary>" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat $folder/*.md >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "</details>" >> $GITHUB_STEP_SUMMARY
        done

  merge_upload_coverage_data:
    needs: build-test
    runs-on: ubuntu-latest
    if: ${{ always() }}
    steps:
    # get coverage only for 1 job 
    - name: Get data
      uses: actions/download-artifact@v4
      with:
        pattern: cov-results-3.13-os-macos-latest  #CHECK: need to be updated when new version are available
    # Upload coverage report for Codecov
    - name: Upload coverage reports to Codecov
      uses: codecov/codecov-action@v5
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        verbose: true
        files: cov-results-3.13-os-macos-latest/cov-3.13-os-macos-latest.xml    #CHECK: need to be updated when new version are available
  